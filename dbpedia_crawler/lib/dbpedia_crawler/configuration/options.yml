crawler:
  crawl_all_ids: true                         # whether to push the initial command on start
  command_retries: 1                          # number of retries for failed commands
  sleep_seconds: 5                            # seconds to sleep if command queue is empty
  types:                                      # types of entities which the crawler can handle
    actor: "http://dbpedia.org/ontology/Actor"
    character: "http://dbpedia.org/ontology/FictionalCharacter"
    director: "http://www.hpi.uni-potsdam.de/semmul2014/lodofmovies.owl#Director"
    episode: "http://schema.rdfs.org/Episode"
    movie: "http://schema.rdfs.org/Movie"
    organisation: "http://schema.rdfs.org/Organisation"
    person: "http://schema.rdfs.org/Person"
    place: "http://dbpedia.org/ontology/Place"
    season: "http://schema.rdfs.org/TVSeason"
    show: "http://schema.rdfs.org/TVSeries"

queue:
  agent_id: "dbpedia"                         # identifier of this crawler (to determine the queues)
  purge: true                                 # whether to purge the queue on start
  purge_retries: 5                            # retries if purge raises exception for very large queue
  purge_sleep_seconds: 10                     # how long to sleep before attempting another purge
  queue: "lom.source"                         # first part of the queues (e.g. for DBpedia movies: "lom.source.dbpedia.movie")

source:
  endpoint: "http://dbpedia.org/sparql"       # URI of the SPARQL endpoint
  page_size: 10000                            # number of IDs queried at a time, <= 50,000 (Ruby.rdf limit)
  query_retries: 5                            # number of retries for failed data retrieval

writer:
  batch_size: 1000                            # number of triples inserted at a time (do not know maximum, use <= 1000)
  endpoint: "http://localhost:8890/sparql"    # URI of the SPARQL endpoint
  graph: "http://example.com/raw/"            # named graph for storing data
